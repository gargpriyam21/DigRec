{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16 , 4) # Set default figure size\n",
    "\n",
    "print(\"Tensorflow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file as readonly\n",
    "h5f = h5py.File('data/SVHN_single_grey.h5', 'r')\n",
    "\n",
    "# Load the training, test and validation set\n",
    "X_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "X_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "X_val = h5f['X_val'][:]\n",
    "y_val = h5f['y_val'][:]\n",
    "\n",
    "# Close this file\n",
    "h5f.close()\n",
    "\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Validation set', X_val.shape, y_val.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = X_train.shape[1]\n",
    "num_channels = X_train.shape[-1]\n",
    "num_classes = y_train.shape[1]\n",
    "num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean on the training data\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "\n",
    "# Calculate the std on the training data\n",
    "train_std = np.std(X_train, axis=0)\n",
    "\n",
    "# Subtract it equally from all splits\n",
    "X_train = (X_train - train_mean) / train_std\n",
    "X_test = (X_test - train_mean)  / train_std\n",
    "X_val = (train_mean - X_val) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, nrows, ncols, cls_true, cls_pred=None):\n",
    "    \"\"\" Plot nrows * ncols images from images and annotate the images\n",
    "    \"\"\"\n",
    "    # Initialize the subplotgrid\n",
    "    fig, axes = plt.subplots(nrows, ncols)\n",
    "    \n",
    "    # Randomly select nrows * ncols images\n",
    "    rs = np.random.choice(images.shape[0], nrows*ncols)\n",
    "    \n",
    "    # For every axes object in the grid\n",
    "    for i, ax in zip(rs, axes.flat): \n",
    "        \n",
    "        # Predictions are not passed\n",
    "        if cls_pred is None:\n",
    "            title = \"True: {0}\".format(np.argmax(cls_true[i]))\n",
    "        \n",
    "        # When predictions are passed, display labels + predictions\n",
    "        else:\n",
    "            title = \"True: {0}, Pred: {1}\".format(np.argmax(cls_true[i]), cls_pred[i])  \n",
    "            \n",
    "        # Display the image\n",
    "        ax.imshow(images[i,:,:,0], cmap='binary')\n",
    "        \n",
    "        # Annotate the image\n",
    "        ax.set_title(title)\n",
    "        \n",
    "        # Do not overlay a grid\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        \n",
    "# Plot 2 rows with 9 images each from the training set\n",
    "plot_images(X_train, 2, 9, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_weight_variable(layer_name, shape):\n",
    "    \"\"\" Retrieve an existing variable with the given layer name \n",
    "    \"\"\"\n",
    "    return tf.get_variable(layer_name, shape=shape, initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "def fc_weight_variable(layer_name, shape):\n",
    "    \"\"\" Retrieve an existing variable with the given layer name\n",
    "    \"\"\"\n",
    "    return tf.get_variable(layer_name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\" Creates a new bias variable\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.constant(0.0, shape=shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input,               # The previous layer\n",
    "                layer_name,         # Layer name\n",
    "                num_input_channels, # Num. channels in prev. layer\n",
    "                filter_size,        # Width and height of each filter\n",
    "                num_filters,        # Number of filters\n",
    "                pooling=True):      # Use 2x2 max-pooling\n",
    "\n",
    "    # Shape of the filter-weights for the convolution\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new filters with the given shape\n",
    "    weights = conv_weight_variable(layer_name, shape=shape)\n",
    "    \n",
    "    # Create new biases, one for each filter\n",
    "    biases = bias_variable(shape=[num_filters])\n",
    "\n",
    "    # Create the TensorFlow operation for convolution\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME') # with zero padding\n",
    "\n",
    "    # Add the biases to the results of the convolution\n",
    "    layer += biases\n",
    "    \n",
    "    # Rectified Linear Unit (RELU)\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Down-sample the image resolution?\n",
    "    if pooling:\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Return the resulting layer and the filter-weights\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input,               # The previous layer\n",
    "                layer_name,         # Layer name\n",
    "                num_input_channels, # Num. channels in prev. layer\n",
    "                filter_size,        # Width and height of each filter\n",
    "                num_filters,        # Number of filters\n",
    "                pooling=True):      # Use 2x2 max-pooling\n",
    "\n",
    "    # Shape of the filter-weights for the convolution\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new filters with the given shape\n",
    "    weights = conv_weight_variable(layer_name, shape=shape)\n",
    "    \n",
    "    # Create new biases, one for each filter\n",
    "    biases = bias_variable(shape=[num_filters])\n",
    "\n",
    "    # Create the TensorFlow operation for convolution\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME') # with zero padding\n",
    "\n",
    "    # Add the biases to the results of the convolution\n",
    "    layer += biases\n",
    "    \n",
    "    # Rectified Linear Unit (RELU)\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Down-sample the image resolution?\n",
    "    if pooling:\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Return the resulting layer and the filter-weights\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(input,        # The previous layer\n",
    "             layer_name,   # The layer name\n",
    "             num_inputs,   # Num. inputs from prev. layer\n",
    "             num_outputs,  # Num. outputs\n",
    "             relu=True):   # Use RELU?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = fc_weight_variable(layer_name, shape=[num_inputs, num_outputs])\n",
    "    biases = bias_variable(shape=[num_outputs])\n",
    "\n",
    "    # Calculate the layer activation\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 32         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 64         # There are 36 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 256            # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, img_size, img_size, num_channels), name='x')\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\n",
    "\n",
    "y_true_cls = tf.argmax(y_true, axis=1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name = 'keep_prob_node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the achitecture is INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL] -> DROPOUT -> [FC -> RELU] -> FC\n",
    "conv_1, w_c1 = conv_layer(input=x,\n",
    "                          layer_name=\"conv_1\",\n",
    "                          num_input_channels=num_channels,\n",
    "                          filter_size=filter_size1,\n",
    "                          num_filters=num_filters1, pooling=True)\n",
    "\n",
    "conv_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2, w_c2 = conv_layer(input=conv_1,\n",
    "                          layer_name=\"conv_2\",\n",
    "                          num_input_channels=num_filters1,\n",
    "                          filter_size=filter_size2,\n",
    "                          num_filters=num_filters2,\n",
    "                          pooling=True)\n",
    "\n",
    "# Apply dropout after the pooling operation\n",
    "dropout = tf.nn.dropout(conv_2, keep_prob)\n",
    "\n",
    "dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(dropout)\n",
    "\n",
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_1 = fc_layer(input=layer_flat,\n",
    "                layer_name=\"fc_1\",\n",
    "                num_inputs=num_features,\n",
    "                num_outputs=fc_size,\n",
    "                relu=True)\n",
    "\n",
    "fc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_2 = fc_layer(input=fc_1,\n",
    "                layer_name=\"fc_2\",\n",
    "                num_inputs=fc_size,\n",
    "                num_outputs=num_classes,\n",
    "                relu=False)\n",
    "\n",
    "fc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(fc_2, name = 'out')\n",
    "\n",
    "# The class-number is the index of the largest element.\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte the cross-entropy# Calcua \n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc_2, labels=y_true)\n",
    "\n",
    "# Take the average of the cross-entropy for all the image classifications.\n",
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global step is required to compute the decayed learning rate# Global \n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "# Apply exponential decay to the learning rate\n",
    "learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.96, staircase=True)\n",
    "\n",
    "# Construct a new Adam optimizer\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class equals the true class of each image?\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "\n",
    "# Cast predictions to float and calculate the mean\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "save_dir = 'checkpoints/'\n",
    "\n",
    "# Create directory if it does not exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "save_path = os.path.join(save_dir, 'svhn_single_greyscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to increment training on existing model\n",
    "saver.restore(sess=session, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training samples in each iteration \n",
    "batch_size = 64\n",
    "\n",
    "# Keep probability in dropout layer\n",
    "dropout = 0.5\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = 0\n",
    "def optimize(num_iterations, display_step):\n",
    "    \n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "    tf.train.write_graph(session.graph_def, 'out',\n",
    "            'svhn_single_greyscale' + '.pbtxt', True)\n",
    "\n",
    "    \n",
    "    for step in range(num_iterations):\n",
    "\n",
    "        offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "        batch_data = X_train[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]        \n",
    "        \n",
    "        feed_dict_train = {x: batch_data, y_true: batch_labels, keep_prob: dropout}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every display_step\n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            # Calculate the accuracy on the training-set.\n",
    "            batch_acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            print(\"Minibatch accuracy at step %d: %.4f\" % (step, batch_acc))\n",
    "            \n",
    "            # Calculate the accuracy on the validation-set\n",
    "            validation_acc = session.run(accuracy, {x: X_val, y_true: y_val, keep_prob: 1.0})\n",
    "            print(\"Validation accuracy: %.4f\" % validation_acc)\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_diff = time.time() - start_time\n",
    "    \n",
    "    # Calculate the accuracy on the test-set    \n",
    "    print(X_test.shape)    \n",
    "    print(y_test.shape)\n",
    "    test_accuracy = session.run(accuracy, {x: X_test, y_true: y_test, keep_prob: 1.0})\n",
    "    \n",
    "    print(\"Test accuracy: %.4f\" % test_accuracy)\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_diff)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[:15000]\n",
    "y_test = y_test[:15000]\n",
    "print(X_test.shape)    \n",
    "print(y_test.shape)\n",
    "optimize(num_iterations=5000, display_step=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to train the model fully, time taken will depend on your CPU/GPU(preferred)\n",
    "\n",
    "#optimize(num_iterations=50000, display_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(sess=session, save_path=save_path)\n",
    "save_path2 = os.path.join('out/', 'svhn_single_greyscale.chkp')\n",
    "saver.save(sess=session, save_path=save_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the model to a pb file\n",
    "\n",
    "MODEL_NAME = 'svhn_single_greyscale'\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "def export_model(input_node_names, output_node_name):\n",
    "    freeze_graph.freeze_graph('out/' + MODEL_NAME + '.pbtxt', None, False,\n",
    "        'out/' + MODEL_NAME + '.chkp', output_node_name, \"save/restore_all\",\n",
    "        \"save/Const:0\", 'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model(['x','keep_prob_node'], 'out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
